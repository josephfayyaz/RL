# ðŸ¦¿ Reinforcement Learning for Custom Hopper with Domain Randomization

This repository presents a comprehensive study on **reinforcement learning (RL)** algorithms applied to a **custom MuJoCo Hopper** environment. Our goal is to develop robust control policies through **domain randomization**, **curriculum learning**, and **policy gradient methods**.

The project combines classic and modern RL techniques:
- **REINFORCE & Actor-Critic** (from-scratch implementations)
- **Proximal Policy Optimization (PPO)** (via Stable-Baselines3)
- **Uniform Domain Randomization (UDR)** and **Entropy-based Curriculum Domain Randomization (ES-CDR)**

---

## ðŸ“ Repository Structure

```bash
.
â”œâ”€â”€ agents/                              # (Optional) future agents folder
â”œâ”€â”€ env/                                 # Custom MuJoCo environment
â”‚   â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ custom_hopper.py                 # Custom Hopper environment with DR
â”‚   â””â”€â”€ mujoco_env.py
â”‚
â”œâ”€â”€ Logs/                                # CSV logs and WandB (if used)
â”‚   â”œâ”€â”€ actor_critic/
â”‚   â”œâ”€â”€ baseline/
â”‚   â”œâ”€â”€ csv/
â”‚   â””â”€â”€ PPO/
â”‚
â”œâ”€â”€ Models/                              # Trained model checkpoints
â”‚   â”œâ”€â”€ actor_critic/
â”‚   â”œâ”€â”€ PPO/
â”‚   â””â”€â”€ reinforce_baseline/
â”‚
â”œâ”€â”€ Render/                              # Optional rendering or video files
â”‚
â”œâ”€â”€ training/                            # Main training scripts
â”‚   â”œâ”€â”€ wandb/                           # WandB config/data (if used)
â”‚   â”œâ”€â”€ PPO_Hyperparameter_Calculation.py
â”‚   â”œâ”€â”€ PPO_UDR_ES_CDR.py
â”‚   â”œâ”€â”€ Train_Actor_Critic.py
â”‚   â”œâ”€â”€ Train_Baseline.py
â”‚   â””â”€â”€ Train_Reinforce_vanila.py
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ metric_extraction.py             # CSV parsing, plotting utilities
â”‚
â”œâ”€â”€ wandb/                               # WandB cache directory (gitignored)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ __init__.py
â””â”€â”€ README.md
```

---

## ðŸ§ª Environments & Randomization

The environment is based on a custom subclass of the MuJoCo Hopper (`custom_hopper.py`), extended with:

- **Parameter Randomization**: friction, damping, body mass, initial state
- **Domain Randomization**:
  - *Uniform DR (UDR)*: randomized every episode
  - *Curriculum DR (ES-CDR)*: difficulty scaled with agent performance and return entropy

---

## ðŸ§  Algorithms Implemented

| Algorithm              | Description                                                         |
|------------------------|---------------------------------------------------------------------|
| **REINFORCE**           | Monte Carlo policy gradient with optional baseline                 |
| **Actor-Critic**        | TD-based policy/value method                                       |
| **PPO**                 | Clipped surrogate objective with GAE (Stable-Baselines3)           |
| **UDR**                 | Domain variation with uniform sampling                             |
| **ES-CDR**              | Return entropy-driven difficulty adjustment                        |

---

## ðŸ› ï¸ Setup Instructions

### âœ… Dependencies (tested on Ubuntu 22.04)

```bash
pip install -r requirements.txt
```

Essential packages:

```text
gym==0.21.0
stable-baselines3==1.7.0
mujoco-py>=2.1,<2.2
cython<3
scipy
Jinja2==3.1.2
importlib-metadata==4.13.0
patchelf
```

Make sure MuJoCo is installed and licensed correctly.

---

## ðŸš€ Training Commands

### ðŸŽ¯ REINFORCE / Actor-Critic

```bash
python training/Train_Reinforce_vanila.py
python training/Train_Actor_Critic.py
python training/Train_Baseline.py
```

### ðŸ¤– PPO with UDR + ES-CDR

```bash
python training/PPO_UDR_ES_CDR.py --seed 0 --train_steps 350000
```

### ðŸ”¬ PPO Hyperparameter Sweep

```bash
python training/PPO_Hyperparameter_Calculation.py
```

---

## ðŸ“Š Logging & Evaluation

- All CSV logs are stored in `Logs/csv/`
- Trained models saved under `Models/`
- You can visualize performance using:

```python
from utils.metric_extraction import plot_training_curve
plot_training_curve("Logs/csv/ppo_run.csv")
```

---

## ðŸ“ˆ Sample Results

| Curriculum Level | Avg Return | Std Dev | Entropy |
|------------------|------------|---------|---------|
| Level 1          | 810        | 60      | 0.95    |
| Level 2          | 720        | 82      | 1.20    |
| Level 3          | 680        | 90      | 1.50    |

---

## ðŸ‘¥ Project Participants

Please list contributors here:

- **Pishool** â€“ Custom environment design, algorithm implementation, evaluation
- *(Add names and roles as needed)*

---

## ðŸ“š References & Acknowledgements

- OpenAI Baselines
- Stable-Baselines3 Docs
- MuJoCo Documentation

---

## ðŸ§  Future Directions

- Add evaluation over unseen dynamics
- Experiment with off-policy algorithms (e.g., SAC, DDPG)
- Integrate video rendering and performance visualizations

---

## ðŸ“¬ Contact

Please reach out via GitHub issues or Linkedin profiles. 
- **Ali Vaezi** - [LinkedIn](https://www.linkedin.com/in/aliivaezii/)
- **Yousef Fayyaz** - [LinkedIn](https://www.linkedin.com/in/yousef-fayyaz-55ab9a255/)
- **Sajjad Shahali** - [LinkedIn](https://www.linkedin.com/in/sajjad-shahali/)
- **Parastoo Hashemi Alvar** - [LinkedIn](https://www.linkedin.com/in/parastoo-hashemi/)
